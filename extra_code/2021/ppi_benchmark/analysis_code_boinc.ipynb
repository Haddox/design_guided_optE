{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make a new set of decoys for PPI decoy discrimination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import `Python` modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import pandas\n",
    "import glob\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "import umap\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set(font_scale=2, style='ticks', palette='colorblind')\n",
    "\n",
    "# custom modules\n",
    "sys.path.append('scripts/')\n",
    "import design_utils\n",
    "\n",
    "resultsdir = '/net/scratch/haddox/2021/ppi_benchmark/results'\n",
    "if not os.path.isdir(resultsdir):\n",
    "    os.makedirs(resultsdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a list of natives to take through the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all natives: 176\n",
      "training natives: 65\n",
      "all validation natives: 111\n",
      "new validation natives: 76\n"
     ]
    }
   ],
   "source": [
    "# Make a list of all natives\n",
    "all_natives = glob.glob('data/natives/*.pdb')\n",
    "all_natives = [os.path.basename(x)[:4] for x in all_natives]\n",
    "print(f\"all natives: {len(all_natives)}\")\n",
    "\n",
    "# Make a list of the 100 natives Hahnbeom and Frank\n",
    "# originally considered for training\n",
    "natives_100 = glob.glob('/home/dimaio/optE2/dualoptE/decoys/docking/*.pdb')\n",
    "natives_100 = [os.path.basename(x)[:4] for x in natives_100]\n",
    "\n",
    "\n",
    "# Make a list of natives used in training\n",
    "alljobs = '/home/haddox/2019/optE_eval/alljobs'\n",
    "with open(alljobs) as f:\n",
    "    lines = f.readlines()\n",
    "training_natives = []\n",
    "for line in lines:\n",
    "    if './run_docking_single.sh' in line:\n",
    "        (cmd, native) = line.strip().split()\n",
    "        training_natives.append(native)\n",
    "print(f\"training natives: {len(training_natives)}\")\n",
    "\n",
    "# Validation natives\n",
    "validation_natives = [\n",
    "    native for native in all_natives\n",
    "    if native not in training_natives\n",
    "]\n",
    "new_validation_natives = [\n",
    "    native for native in validation_natives\n",
    "    if native not in natives_100\n",
    "]\n",
    "print(f\"all validation natives: {len(validation_natives)}\")\n",
    "print(f\"new validation natives: {len(new_validation_natives)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract PDBs from silent files containing the output of Hahnbeom's global-docking runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Extract all PDBs from a silent file with output from\n",
    "# global docking\n",
    "for native in all_natives:\n",
    "    if native in training_natives:\n",
    "        continue\n",
    "    silent_file = os.path.join(\n",
    "        '/net/scratch/haddox/2021/ppi_benchmark/data/comb1000/',\n",
    "        f'{native}.1000.rescore.out'\n",
    "    )\n",
    "    if not os.path.isfile(silent_file):\n",
    "        continue\n",
    "    output_dir = f'{resultsdir}/global_docks/{native}/'\n",
    "    if not os.path.isdir(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "        # Extract PDBs\n",
    "        print(f'Extracting PDBs for {native}')\n",
    "        design_utils.extract_pdbs_from_silent_file(\n",
    "            working_dir=output_dir,\n",
    "            silent_file=silent_file\n",
    "        )\n",
    "\n",
    "        # Copy the PDB of the native to the same directory as the\n",
    "        # global docks\n",
    "        print(\"Copying native PDB to same directory\")\n",
    "        native_pdb = f'data/natives/{native}_bound_native.pdb'\n",
    "        new_pdb = os.path.join(output_dir, os.path.basename(native_pdb))\n",
    "        ! cp {native_pdb} {new_pdb}\n",
    "        time.sleep(60)\n",
    "\n",
    "        # Strip PDBs of all hydrogens to avoid bug with hydrogen\n",
    "        # placement from old silent files\n",
    "        print(\"Stripping hydrogens from all PDBs\")\n",
    "        pdbs = glob.glob(os.path.join(output_dir, '*.pdb'))\n",
    "        for pdb in pdbs:\n",
    "            cmd = f'scripts/stripH_inplace.pl {pdb}'\n",
    "            ! {cmd}\n",
    "    else:\n",
    "        pdbs = glob.glob(os.path.join(output_dir, '*.pdb'))\n",
    "        #print(native, len(pdbs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make jittered version of each global dock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define parameters for jittering inputs\n",
    "run_dict = {\n",
    "    1 : {\n",
    "        'internal_sampes' : 25, # number of jitters to make\n",
    "        'cluster_rmsd' : 0.2, # how tightly to cluster jitters\n",
    "        'max_translate' : 2, # maximum translation distance\n",
    "        'max_angle' : 1 # maximum angle for rotating things\n",
    "    },\n",
    "    2 : {\n",
    "        'internal_sampes' : 25, # number of jitters to make\n",
    "        'cluster_rmsd' : 0.2, # how tightly to cluster jitters\n",
    "        'max_translate' : 2, # maximum translation distance\n",
    "        'max_angle' : 10 # maximum angle for rotating things\n",
    "    }\n",
    "}\n",
    "\n",
    "# Cycle over all input PDBs and jitter chain B\n",
    "boinc_tag = 'HKH_2021'\n",
    "for native in validation_natives: # natives:\n",
    "    \n",
    "    # I have already generated all jitters\n",
    "    continue\n",
    "    \n",
    "    pdb_dir = f'{resultsdir}/global_docks/{native}/'\n",
    "    pdbs = glob.glob(os.path.join(pdb_dir, '*.pdb'))\n",
    "    if len(pdbs) == 0:\n",
    "        continue\n",
    "    file_listing_pdbs = os.path.join(pdb_dir, 'pdbs.txt')\n",
    "    if not os.path.isfile(file_listing_pdbs):\n",
    "        with open(file_listing_pdbs, 'w') as f:\n",
    "            for pdb in pdbs:\n",
    "                f.write(f'{pdb}\\n')\n",
    "    env_name = 'high_throughput_design_for_E_function_opt'\n",
    "    runs = [1, 2]\n",
    "    for run in runs:\n",
    "\n",
    "        # Assemble command-line argument\n",
    "        output_dir = os.path.join(\n",
    "            pdb_dir,\n",
    "            'jittered_poses',\n",
    "            f'run_{run}/'\n",
    "        )\n",
    "        if not os.path.isdir(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "        cmd = ' '.join([\n",
    "            'python',\n",
    "            'scripts/jitter_binder.py',\n",
    "            file_listing_pdbs,\n",
    "            f'-output_dir {output_dir}',\n",
    "            f'-output_pdb_prefix {boinc_tag}_{native}_run_{run}_',\n",
    "            f\"-internal_samples {run_dict[run]['internal_sampes']}\",\n",
    "            f\"-cluster_rmsd {run_dict[run]['cluster_rmsd']}\",\n",
    "            f\"-max_translate {run_dict[run]['max_translate']}\",\n",
    "            f\"-max_angle {run_dict[run]['max_angle']}\",\n",
    "        ])\n",
    "        cmd = 'source activate {0}'.format(env_name) + '\\n' + cmd\n",
    "\n",
    "        # Carry out the job\n",
    "        sbatch_file_name = os.path.join(\n",
    "            output_dir,\n",
    "            'jitter_binder.sbatch'\n",
    "        )\n",
    "        if not os.path.isfile(sbatch_file_name):\n",
    "            print(native, f'run_{run}', len(pdbs))\n",
    "            design_utils.WriteSbatchFile(\n",
    "                sbatch_file_name,\n",
    "                command=cmd,\n",
    "                queue_type='medium',\n",
    "                memory='2g'\n",
    "            )\n",
    "            ! sbatch {sbatch_file_name}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the beta_soft score function to score global docks before and after jittering to identify a subset to relax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Score global docks before jittering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score global docks before jittering\n",
    "score_app_path = \\\n",
    "    '/home/haddox/Rosetta/main/source/bin/score.linuxgccrelease'\n",
    "for native in validation_natives: # natives:\n",
    "    \n",
    "    # I have already scored all jitters\n",
    "    continue\n",
    "    \n",
    "    pdb_dir = f'{resultsdir}/global_docks/{native}/'\n",
    "    if not os.path.isdir(pdb_dir):\n",
    "        continue\n",
    "    pdbs = glob.glob(os.path.join(pdb_dir, '*.pdb'))\n",
    "    file_listing_pdbs = os.path.join(pdb_dir, 'pdbs.txt')\n",
    "    if not os.path.isfile(file_listing_pdbs):\n",
    "        with open(file_listing_pdbs, 'w') as f:\n",
    "            for pdb in pdbs:\n",
    "                f.write(f'{pdb}\\n')\n",
    "    extra_args = ['-beta']\n",
    "    output_dir = pdb_dir\n",
    "    scores_file_prefix = os.path.join(output_dir, 'soft_rep_score')\n",
    "    scores_sbatch_file = scores_file_prefix + '.sbatch'\n",
    "    if not os.path.isfile(scores_sbatch_file):\n",
    "        design_utils.compute_score_using_rosettascripts(\n",
    "            score_app_path=score_app_path,\n",
    "            file_listing_pdbs=file_listing_pdbs,\n",
    "            weights_file='beta_soft',\n",
    "            extra_args=extra_args,\n",
    "            output_dir=output_dir,\n",
    "            scores_file_prefix=scores_file_prefix,\n",
    "            submit_sbatch_job=True,\n",
    "            queue_type='long',\n",
    "            memory='2g'\n",
    "        )\n",
    "\n",
    "    # And score docks after jittering\n",
    "    for run in [1, 2]:\n",
    "        run_dir = os.path.join(\n",
    "            pdb_dir,\n",
    "            'jittered_poses',\n",
    "            f'run_{run}/'\n",
    "        )\n",
    "        pdbs = glob.glob(os.path.join(run_dir, '*.pdb'))\n",
    "        if len(pdbs) == 0:\n",
    "            continue\n",
    "        file_listing_pdbs = os.path.join(run_dir, 'pdbs.txt')\n",
    "        if not os.path.isfile(file_listing_pdbs):\n",
    "            with open(file_listing_pdbs, 'w') as f:\n",
    "                for pdb in pdbs:\n",
    "                    f.write(f'{pdb}\\n')\n",
    "        extra_args = ['-beta']\n",
    "        output_dir = run_dir\n",
    "        scores_file_prefix = os.path.join(output_dir, 'soft_rep_score')\n",
    "        scores_sbatch_file = scores_file_prefix + '.sbatch'\n",
    "        if not os.path.isfile(scores_sbatch_file):\n",
    "            design_utils.compute_score_using_rosettascripts(\n",
    "                score_app_path=score_app_path,\n",
    "                file_listing_pdbs=file_listing_pdbs,\n",
    "                weights_file='beta_soft',\n",
    "                extra_args=extra_args,\n",
    "                output_dir=output_dir,\n",
    "                scores_file_prefix=scores_file_prefix,\n",
    "                submit_sbatch_job=True,\n",
    "                queue_type='long',\n",
    "                memory='2g'\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in scores from above and pick the 5 best-scoring jitters per decoy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Cycle through natives one at a time\n",
    "for native in new_validation_natives:\n",
    "\n",
    "    # I have already selected best-scoring jitters\n",
    "    continue\n",
    "    \n",
    "    # Read in scores for global docks before jittering\n",
    "    dfs = []\n",
    "    score_file = f'{resultsdir}/global_docks/{native}/soft_rep_score.sc'\n",
    "    if not os.path.isfile(score_file):\n",
    "        print(f\"Missing scores for {native}\")\n",
    "        continue\n",
    "    df = pandas.read_csv(score_file, sep='\\s+')\n",
    "    df.dropna(subset=['description'], inplace=True)\n",
    "    df['score'] = df['score'].astype(float)\n",
    "    df['pdb'] = df['description'].apply(\n",
    "        lambda x: x[:-5] + '.pdb'\n",
    "    )\n",
    "    df['native'] = native\n",
    "    df['global_dock_parent'] = df['pdb'].apply(\n",
    "        lambda x: os.path.basename(x).replace('.pdb', '')\n",
    "    )\n",
    "    df['original_dock'] = True\n",
    "    del df['SCORE:']\n",
    "    dfs.append(df)\n",
    "\n",
    "    # Read in scores for global docks after jittering\n",
    "    jitter_dir = f'{resultsdir}/global_docks/{native}/jittered_poses/'\n",
    "    score_files = glob.glob(os.path.join(\n",
    "        jitter_dir,\n",
    "        'run*/soft_rep_score.sc'\n",
    "    ))\n",
    "    for score_file in score_files:\n",
    "        df = pandas.read_csv(score_file, sep='\\s+')\n",
    "        df.dropna(subset=['description'], inplace=True)\n",
    "        df['score'] = df['score'].astype(float)\n",
    "        df['pdb'] = df['description'].apply(\n",
    "            lambda x: x[:-5] + '.pdb'\n",
    "        )\n",
    "        df['native'] = native\n",
    "        df['global_dock_parent'] = df['pdb'].str.extract(\n",
    "            r'.+HKH_2021_\\w+_run_[\\d]_(.+)_chain_.+pdb'.format(native)\n",
    "        )\n",
    "        df['original_dock'] = False\n",
    "        del df['SCORE:']\n",
    "        dfs.append(df)\n",
    "    soft_scores_df = pandas.concat(dfs, sort=False)\n",
    "    soft_scores_df['pdb_bn'] = \\\n",
    "        soft_scores_df['pdb'].apply(os.path.basename)\n",
    "    #soft_scores_df = soft_scores_df[\n",
    "    #    ~soft_scores_df['global_dock_parent'].str.contains('_S_')\n",
    "    #]\n",
    "    del dfs\n",
    "    if len(soft_scores_df) < 25000:\n",
    "        print(native, len(soft_scores_df), 'continueing')\n",
    "        continue\n",
    "    print(native, len(soft_scores_df))\n",
    "\n",
    "    # For each parent from the global dock, select a few\n",
    "    # of the best-scoring structures for relaxing\n",
    "    boinc_relax_dir = f'{resultsdir}/global_docks/{native}/boinc_relax/'\n",
    "    home_boinc_relax_dir = boinc_relax_dir.replace(\n",
    "        '/net/scratch/', '/home/'\n",
    "    )\n",
    "    for d in [boinc_relax_dir, home_boinc_relax_dir]:\n",
    "        if not os.path.isdir(d):\n",
    "            os.makedirs(d)\n",
    "    parents = list(set(soft_scores_df['global_dock_parent']))\n",
    "    soft_scores_df.sort_values('score', inplace=True)\n",
    "    n_candidates_per_parent = 5\n",
    "    for parent in parents:\n",
    "\n",
    "        # Make a list of the best one or few of the best-scoring\n",
    "        # decoys\n",
    "        pdbs = list(soft_scores_df[\n",
    "            soft_scores_df['global_dock_parent'] == parent\n",
    "        ]['pdb'][:n_candidates_per_parent])\n",
    "\n",
    "        # Copy PDBs to a new directory\n",
    "        for pdb in pdbs:\n",
    "            assert os.path.isfile(pdb), pdb\n",
    "\n",
    "            # Copy PDB to directory on /net/scratch\n",
    "            pdb_bn = os.path.basename(pdb)\n",
    "            for d in [boinc_relax_dir]: # home_boinc_relax_dir\n",
    "                if 'jittered_poses' in pdb:\n",
    "                    new_pdb = os.path.join(d, pdb_bn)\n",
    "                else:\n",
    "                    new_pdb = os.path.join(\n",
    "                        d,\n",
    "                        f'{boinc_tag}_{native}_{pdb_bn}'\n",
    "                    )\n",
    "                if not os.path.isfile(new_pdb):\n",
    "                    ! cp {pdb} {new_pdb}\n",
    "\n",
    "    # Make 10 copies of the native PDB\n",
    "    native_pdb = f'data/natives/{native}_bound_native.pdb'\n",
    "    for i in range(10):\n",
    "        pdb_bn = os.path.basename(native_pdb).replace(\n",
    "            '.pdb', f'_{i}_xtal.pdb'\n",
    "        )\n",
    "        for d in [boinc_relax_dir, home_boinc_relax_dir]:\n",
    "            new_pdb = os.path.join(\n",
    "                d,\n",
    "                f'{boinc_tag}_{pdb_bn}'\n",
    "            )\n",
    "            if not os.path.isfile(new_pdb):\n",
    "                ! cp {native_pdb} {new_pdb}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submit jobs to boinc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1JTG 5010\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "1Z0K 5010\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "1FCC 5010\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "1GHQ 5010\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "Missing PDBs for 1N2C\n",
      "1HE8 5010\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "1HIA 5010\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "1ACB 5010\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "1R8S 5010\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "1PVH 5010\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "1KXP 5010\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "1OFU 5010\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "1JPS 5010\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "1E6J 5010\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "4CPA 5010\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "1NW9 5010\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "1K4C 5010\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "1VFB 5010\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "1AK4 5010\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "1LFD 5010\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "1XD3 5010\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "1EER 5010\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "1CGI 5010\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "Missing PDBs for 1BGX\n",
      "1I4D 5010\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "2FJU 5010\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "1ML0 5010\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "2IDO 5010\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "BOYV 5010\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "Missing PDBs for 2HMI\n",
      "1KAC 5010\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "1S1Q 5010\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "1OYV 5015\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "2VDB 5015\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "1RLB 5015\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "2Z0E 5015\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "1ZLI 5015\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "1MLC 5015\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "1SBB 5015\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "1JK9 5015\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "2OOR 5010\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "2OT3 5015\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "1N8O 5015\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "1E96 5015\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "2OZA 5015\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "1H9D 5015\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "1D6R 5011\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "1E4K 5015\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "1GXD 5015\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "1BVN 5015\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "2B42 5015\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "1GL1 5015\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "Missing PDBs for 1WDW\n",
      "2FD6 5015\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "1MQ8 5015\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "1FLE 5015\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "2HLE 5015\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "Missing PDBs for 1DE4\n",
      "1KTZ 5015\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "2B4J 5015\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "1T6B 5015\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "1WEJ 5015\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "1GP2 5015\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "1PXV 5015\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "2J7P 5015\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "3BP8 5015\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "1R0R 5015\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "2C0L 5015\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "1QFW 5015\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "1I2M 5015\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "1IRA 5015\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "2OUL 5015\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "1I9R 5015\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "Missing PDBs for 1ZM4\n",
      "1KLU 5015\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "1TMQ 5015\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n"
     ]
    }
   ],
   "source": [
    "for native in new_validation_natives:\n",
    "\n",
    "    # Get inputs\n",
    "    boinc_relax_dir = f'{resultsdir}/global_docks/{native}/boinc_relax/'\n",
    "    flags_file = 'scripts/HKH_2021_boinc_ppi_relax_flags.txt'\n",
    "    flags_file_bn = os.path.basename(flags_file)\n",
    "    new_flags_file = os.path.join(boinc_relax_dir, flags_file_bn)\n",
    "    if not os.path.isfile(new_flags_file):\n",
    "        print(new_flags_file)\n",
    "        ! cp {flags_file} {new_flags_file}\n",
    "    pdbs = glob.glob(os.path.join(boinc_relax_dir, '*.pdb'))\n",
    "    if len(pdbs) == 0:\n",
    "        print(f'Missing PDBs for {native}')\n",
    "        continue\n",
    "\n",
    "    # Cycle through PDBs and submit relax job for each one\n",
    "    print(native, len(pdbs))\n",
    "    for (i, pdb) in enumerate(pdbs):\n",
    "\n",
    "        if i % 1000 == 0:\n",
    "            print(i)\n",
    "\n",
    "        # Make a submission file\n",
    "        inputfile = os.path.basename(pdb)\n",
    "        name = inputfile.replace('.pdb', '')\n",
    "        resultfile = 'default.out.gz'\n",
    "        boinc_submit_template = 'scripts/boinc_submit.txt'\n",
    "        with open(boinc_submit_template) as f:\n",
    "            protocol = f.read()\n",
    "        protocol = protocol.replace('%%name%%', name)\n",
    "        protocol = protocol.replace('%%description%%', name)\n",
    "        protocol = protocol.replace('%%inputfile%%', inputfile)\n",
    "        protocol = protocol.replace('%%resultfile%%', resultfile)\n",
    "        boinc_submit_file = os.path.join(\n",
    "            boinc_relax_dir,\n",
    "            f'{name}.boinc_submit.txt'\n",
    "        )\n",
    "        if not os.path.isfile(boinc_submit_file):\n",
    "            with open(boinc_submit_file, 'w') as f:\n",
    "                f.write(protocol)\n",
    "\n",
    "            # Submit job to boinc\n",
    "            cmd = ' '.join([\n",
    "                '/projects/boinc/bin/boinc_submit',\n",
    "                os.path.basename(boinc_submit_file)\n",
    "            ])\n",
    "            boinc_job_outfile = \\\n",
    "                boinc_submit_file.replace('.txt', '.out')\n",
    "            process = subprocess.Popen(\n",
    "                cmd,\n",
    "                stdout=subprocess.PIPE,\n",
    "                cwd=boinc_relax_dir,\n",
    "                shell=True\n",
    "            )\n",
    "            (out, err) = process.communicate()\n",
    "            with open(boinc_job_outfile, 'wb') as f:\n",
    "                f.write(out)\n",
    "            if err:\n",
    "                print(err)\n",
    "                raise ValueError('Error\\n{0}'.format(err))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (high_throughput_design_for_E_function_opt)",
   "language": "python",
   "name": "high_throughput_design_for_e_function_opt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
